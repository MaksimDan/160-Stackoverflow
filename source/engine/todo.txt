# questions for discussion for nick and duncan
* automatic vector weight tuning
* feature scaling and which one we should chose
* minor changes in the weight vector, had very small consequences in determining the overall ranks
* using the stat server speeding up queries (perhaps not a concern anymore)
* how we can no longer use votes


# duncans advice round 2
* focus on on the big picture, and what you will deliver - not the methods, or performance
* make the questions random


# future assign tasks
* build the indicator pickle file -> sengmi 


# todo
* get esmonds harddrive
* add threshold error
* visualization problem with multiple user activity.
	* problem with multiple people ON TYPE -> needs to be made unique
	* otherwise this will produce a larger error since it is counted twice
* the current debug errors? They are currently happening because there is mismatch between between the user ids between what we observe in y.csv and all the users. this means that either all_users does not encapsulate all users (doubt it), or that the observed users in y.csv are incorrect. Look at the debug log and see if the users exist in the list. 
* reread paper
* negative weights?
* remap users list only from all the possible responders that you measure
* complete Cartesian system for weight
* progressbar - all custom titles, flush to stream
* perhaps try to use different scalers and see how error responds
* automatic vector weight tuning
	* output a log file that reports the a history of each weight (mapped with its associated feature), and how what weights contributed to how much of the total error individually.
* rerun user expertise network, as it does not seem to be currently inline with X.csv ... (user id key errors)
* throughly comment the code description within egnine
* move archive folder to outside eda
* document components of the engine on google doc proposal
	* residual matrix difficulties	

	
# feature features
*  Additionally incorporate the frequency for each tag of every user for users.csv as another feature
* engineered feature: users compatability. Identify which users have activities together (comments, postids, upvotes, downvotes, answers, favorites) -> these are defined as user-co-occurances. Incorporate this as part of the user similiarity matrix. This users are more likely to come accross the same question. Users that comments together tend to be available together.
* taking into account user reputation per tag. (kinda overlaps with other features though)
* general popularity of tag


where we match a users schedule with the creationdate of the post
since most users tend to answer around that time (early on in the post) (edited)
but in reality people can also answer whenever they want
so we might been a better way to comprehend this.


# ideas
* From the tag network, using spectral clustering to further catagorize the tags into groups.
* consider collaborative filtering as another feature (see one note, and there is a bookmarked youtube video on it).


# thoughts
* tag network: We run spectral clustering [18] on the tag graph using normalized cut criteria. This leads to a partitioning of tags into several clusters, where each cluster is interpreted as a unique topic. using LDA


# paper
* collaboration - more answer, more comments == more lasting value to thread (number of unique views)
* so therefore group together a team of people to write together a response


metrics:
compatability
topical expertise
availability


# duncans advice round 1:
* Formalize what you mean by an error. Here are some of the ways we can quantify error:
	* How many do we get right?
	* How many did we get right in the top 10, top 100?
	* Make sure to measure the error in multiple ways. How we define the error depends one our purpose? Our purpose could mean trying identify as many people as possible, in the hopes that at least one person would answer this question. Then our error metric in this case likely be using a threshold as described below.	However, it is also good to consider multiple types of error and what they measure.
	* Rank error from recommender system.
		* Model: ignore user(s) by threshold.
		* If user does not appear top x% of the recommender system, then ignore the user. Then the accuracy metric would be the number of people we got right. For example, 100% acceptance would mean 100% accurate model (not a good model, but still a model), but a 50% acceptance threshold might mean only 20% success rate.
* What does it mean when a user answered a question? edited, commented, or answered the question, upvoted, downvoted, or favorited (any activity in general). These need to all be handled a bit differently.
* IMPORTANT: Also focus on some RESIDUAL(error) analysis: identify what percent of people were not on stackoverflow when the recommender system recommended the question to them. After formalizing the error. Take a look at the characteristics of that error. Can we identify a group of people in that error that we happen to get wrong all the time? This will help direct us in engineering more features that can minimize the error. Duncan recommends that this is where we should spend MOST of our time on the project.
	* User availability
	* How many people did the recommender system recommend, but they happened to have created there account after the fact? (see indicator variables).
	* Why dont the people show up in our rankings?
		* Use this to justify what threshold to chose. For example, if 20% of people dont see the answer as determined by the recommender system, then it would make sense to ignore 20% of the poeple as selected by the recommender system.
	* How many people dont seem to be active (latency) at the time of the question was posted?
	* How many people could have answered?
* More eda, respect the data, look at the data, dont fight it the data.
	* Will be useful in deciding the features.
* End goal: understand what you are doing do you can explain it.


# archive
* the questions do not necessarily have to be unanswered. We are strictly just routing questions, which as a use case may be used to route users to unanswered questions. However, purely from a educational perspective, these are the samething.
